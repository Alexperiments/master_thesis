ETSR:
	Conv-3
	! LCB(lightweight CNN backbone):
		! HPB(high-preserving block):
			! HFM(high-frequency filtering module):
				averagepool
				upsample (pixel shuffle)
				subtract upsampled from input
			! ARFB(Adaptive residual feature block):
				RU(residual units):
					Reduction	
					Expansion
				Conv-1
				Conv-3
			! downsample (forse averagepool)
			! upsample (bilinear)
			! Conv-1
			! CA (Channel Attention)
	LTB(lightweight transformer backbone):	
		! ET(efficient transformer):
			Layer normalization
			! EMHA(efficient multi-head attention):
				Reduction
				Linear (input to 3 neuron)
				! Feature Split (project the 3 neuron m times)
				! Scaled dot-product:
					MatMul 
					Scale
					SoftMax
				Expansion
			! MLP(multi-layer percept?)
		Embedding
	
	
Ogni Conv layer ha kernel_size=3 (eccetto per il modulo Reduction che è 1x1) e num_channels=32 (eccetto per il fusion layer, che ne ha 64)
Per fare upscale usano pixelshuffle
In HFM->averagepool k=2
Ci sono 3 HPB nel LCB
C'è 1 ET nel LTB
Lo splitting factor in ET è s=4 (splitting factor per dividere le Q,V,K in sezioni più piccole)
Nel pre-post processing di ET k=3, m=8 (m è il numero di teste del EMHA)
Initial learnable weight in ARFB set to 1
Channel attention f2 performs L2 normalization within all channels for each spatial position to remove spatial information: f2(x_ijc) = x_ijc / ||x_ij||_c (B, c, i, j) -> 


Cosa sono i Reduction e Expansion layers??
Dove viene definito il Residual Scaling with Adaptive weights (RSA)?
	Probabilmente le Residual Units con RSA citate nell'articolo si riferiscono a quelle introdotte
	in Wang et al. ("Lightweight Image Super-Resolution with Adaptive Weighted Learning Network"). Se
	questo fosse il caso Reduction = Conv-1 + ReLU, Expansion = Conv-1. E il RSA è semplicemente la
	struttura dei pesi.

Cos'è il fusion layer?
	Penso sia il punto in cui concatenano le due residual units e applicano un conv layer, che quindi
	non avrà 32 channel ma 64, come suggerito anche dalla Fig. 5 del paper.

Come si condividono i pesi tra gli ARFB blocks?
	forse semplicemente usando la stessa istanza di ARFB...

Scaled dot-product:
	softmax( (Q * K)/sqrt(embedding size) ) * V

Come si trainano i learnable weights nel ARFB?

Leak factor nel ReLU?

Adam optimizer betas?

Usare regolarizzazioni per l'Adam?

